---
title: "LLMとRAG盲信への警鐘"
emoji: "⚠️"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["LLM", "RAG", "ナレッジグラフ", "アーキテクチャ"]
published: false
---

## はじめに：LLM + RAG は「銀の弾丸」ではありません

※ 本記事は、すでに LLM / RAG を用いた PoC や小規模プロダクトを経験し、「そろそろ限界や違和感が見えてきた」と感じている中級レベル以上のエンジニアを想定しています。  
 一般向けの入門ではなく、実運用を前提にどこまでを LLM / RAG に任せ、どこから先を別レイヤに逃がすべきかを整理することを目的としています。

ここ数年、プロダクトや社内ツールの文脈で、

- 「LLM に RAG をかませれば正確になる」
- 「ベクトル DB にマニュアルを全部突っ込めば、あとは自然言語で検索できる」

という期待が広がっています。

しかし、**LLM と RAG は構造的に「真実性や整合性を保証する仕組み」を持ちません**。  
うまく動いているように見えるケースがあっても、それは「正確性が保証されている」わけではありません。

この記事の目的は次の 3 点です。

- LLM / RAG を盲信してはいけない**構造的理由**を整理すること
- **どこから先が LLM / RAG の危険領域になるのか**を明確にすること
- **KG・SQL・ルールエンジンなど形式的レイヤとの組み合わせが不可欠である理由**を示すこと

なお、本記事は「RAG を使うべきではない」という主張ではありません。  
むしろ **どの領域なら LLM / RAG を安全に活用できるか（Safe Zone）** を明確にすることが狙いです。

また、KG やナレッジモデリングの基礎については、以下の既存記事で解説しています。

- ナレッジグラフ入門  
  https://zenn.dev/knowledge_graph/articles/knowledge-graph-intro
- RAG なしで始めるナレッジグラフ QA  
  https://zenn.dev/knowledge_graph/articles/kg-no-rag-starter
- GenAI Divide とナレッジグラフ  
  https://zenn.dev/knowledge_graph/articles/genai-divide-knowledge-graph

本記事ではそれらと重複しないように、LLM/RAG の「限界」と「設計思想」に絞って解説します。

形式レイヤの詳細な設計・実装については、続編「LLM/RAG の曖昧性を抑える『形式レイヤ』の実装ガイド」 (/Users/takanorisuzuki/work/projects/tech-blog/articles/formal-layer-llm-rag-2025-11.md) で解説しています。

---

## LLM の限界：もっともらしさを最適化する装置

LLM は「理解している」ように見えることがありますが、内部で行っているのは次の処理です。

> コンテキストから次に出現しそうなトークンの確率分布を推定し、その中からもっともらしい系列を選ぶ

これは **「正しい回答を生成するための仕組み」ではありません**。

- 真偽の概念は扱いません
- 整合性を検証する機構もありません
- もっともらしい文章を出力しようとする性質上、「それっぽい誤り」を生成します

LLM 幻覚については、近年の多数のサーベイ（例：A Survey on Hallucination in Large Language Models, 2024）でも構造的課題として整理されています。

---

## RAG の限界：Embedding は「意味」ではなく「統計パターン」

RAG は次のステップで動作します。

1. 質問を Embedding に変換する
2. ベクトル DB で近い文書を取得する
3. LLM に回答を生成させる

しかし、Embedding が表しているのは **人間にとっての「意味そのもの」ではなく、大量のテキストの中での使われ方のパターンをざっくり圧縮したベクトル表現**です。

```mermaid
flowchart LR
    A[生テキスト] --> B[トークナイズ（サブワード分割）]
    B --> C[巨大コーパスでの共起統計の学習]
    C --> D[分布仮説に基づく埋め込み学習]
    D --> E[高次元ベクトル空間 Embedding]

    subgraph 注意
        W[ここでは意味や論理や真偽は扱っていない]
    end

    E -. 人間がそう見なす .-> F[「意味が近い」ように見える]
    W --- C
    W --- D
```

Embedding の距離が近くても、それは「人間にとっての意味が近い」こととイコールではありません。

なお、実際の学習過程ではトランスフォーマや自己教師あり学習、コントラスト学習のような手法が使われますが、本記事では上級者向けの話題として扱いません。ここでは「大量テキストの使われ方パターンを圧縮したベクトル」くらいにラフに理解してもらえれば十分です。

- 最新かどうか
- 文脈が正しいか
- 製品名や番号が正しいか

こうした**正しさの観点は一切評価していません**。  
RAG の限界や評価方法については、近年のサーベイでも「retrieval の揺らぎ」と「LLM 側の幻覚」が複合的に影響することが指摘されています（例：A Survey on Retrieval-Augmentation Generation, 2025）。

これは、その後の Retrieval で生じる曖昧性につながる基盤的な問題です。

現場では、この問題を軽減するために、

- インデックス時や検索時に **メタデータフィルタ**（日付・プロジェクトなど）を必須にする
- 類似度だけでなく、別モデルやルールによる **リランキング** を挟む

といった工夫を行います。  
ただし、これらは「改善策」であり、**問題を完全に解消するものではありません**。

---

## RAG が曖昧さを「緩和」ではなく「増幅」してしまう理由

RAG は「幻覚を抑える」と紹介されがちですが、実際には構造的に **幻覚を増幅しうるメカニズム** を持っています。

```mermaid
flowchart LR
    Q[ユーザ質問] --> VQ[Embedding化された質問ベクトル]
    VQ --> RET[類似度ベースのベクトルDB検索]

    subgraph RetrievalLayer[Retrieval層の曖昧さ]
        RET --> C1[関連が薄いが表現が似ている文書A]
        RET --> C2[古い情報を含む文書B]
        RET --> C3[別ドメインだが用語が似ている文書C]
    end

    C1 --> CTX[LLMに渡されるコンテキスト]
    C2 --> CTX
    C3 --> CTX

    subgraph LLMLayer[LLMの曖昧さ]
        CTX --> L[LLM推論エンジン]
        L --> AOUT[一見それっぽい回答]
    end

    AOUT --> TRUST[根拠付きで正しそうに見える]
```

- Retrieval 層は「似ている文書」を返します
- LLM はそれを「正しい前提」として合成します
- UI 上では「根拠付き回答」に見えます

結果、**誤情報が“もっともらしく補強される”構造**になります。

この構造的揺らぎのため、実運用では必然的に“根拠リンクを提示して人間が確認する”運用が必要になります。

実務では、

- コーパスをドメイン単位で分離する
- メタデータフィルタや再ランキングを強制する
- 「この種の質問は RAG を通さない」といったガードレールを設ける

ことで、この増幅を抑え込むことができます。  
ただし **「とりあえず全部 RAG に投げる」構成は危険です**。  
中級者以上のエンジニアは、質問タイプごとに「RAG を使う・使わない」の線引きを設計段階で定義しておくと安全です。

---

## RAG では「根拠リンクを人間が確認する」運用は構造的に必須

先ほど述べた Retrieval と LLM の構造的曖昧性の結果として、

RAG ベースの QA を運用すると、最終的にこうなります。

1. LLM が回答を生成
2. 「参照した根拠リンク」が提示される
3. 人間がリンクを開き、内容が正しいか確認する

自動評価や自己検査系の手法でかなり改善できますが、**現状の到達点としては、ほとんどのユースケースで人間による最終確認が必要になる場面を完全には避けられません。** 一方で、ドメインが十分に限定され、かつ誤答のコストが低い軽量タスクであれば、RAG ベースの自動応答だけで回している事例も存在します。

（ここで挙げた用語は、ざっくり「**LLM 同士で相互チェックする**」「**怪しいときはスコアを下げる**」といった自動チェックだと思ってもらえれば十分です。詳細を覚える必要はありません。）

- 検索が統計的
- LLM が前提を検証しない
- 整合性の自動検証機構が存在しない

したがって、**現実的には「人間が最終審査を行う」前提で設計しておくのが安全です。**

---

## どこから先は LLM / RAG だけで頑張るべきではないか

LLM/RAG では危険となる領域：

- 契約・法務・ポリシー解釈
- 金額・数量・台帳データ
- アクセス制御・承認ロジック
- SLA やペナルティが発生する判断

これらは**誤るコストが高く、LLM/RAG の特性と相性が悪い領域**です。

初心者のうちは、まず「この質問に間違って答えたら、誰がどれくらい困るか？」をざっくり想像してみるだけでも十分です。お金・契約・権限・締切が絡むものは、だいたい要注意ゾーンだと思ってください。

どちらか迷うときは、**常に危険側に倒す**くらいでちょうど良いです。  
迷うということは、誤答した際に誰かが困る可能性があるケースであることが多いためです。

### RAG Safe / Unsafe 判断フロー（実務向けの簡易版）

```mermaid
flowchart TD
    Q[この質問/処理] --> C1{金額・契約・権限・締切に直結するか?}
    C1 -->|Yes| Unsafe1["Unsafe:<br>LLM/RAGだけでは禁止<br>形式レイヤや明示ルールが必須"]
    C1 -->|No| C2{誤答にすぐ気づけるログ/レビューがあるか?}
    C2 -->|No| Unsafe2["Unsafe寄り:<br>少なくとも人間の最終確認を前提に"]
    C2 -->|Yes| C3{誤答しても致命的でないか?}
    C3 -->|No| Unsafe3["Borderline:<br>事実部分は形式レイヤで固定"]
    C3 -->|Yes| Safe["Safe寄り:<br>RAG/LLMで自動応答してよい"]
```

実案件では、例えば次のような線引きが現実的です。

- 料金計算や請求金額の決定 → SQL / 会計システムを唯一の真実として扱い、LLM は説明だけに使う
- SLA に影響する優先度判定 → ルールエンジンやワークフローエンジンに任せ、LLM は補足説明のみ
- ヘルプセンターの記事案内や、「どのドキュメントを読むべきか」の推薦 → Safe 寄りの領域として RAG を使う

---

## LLM/RAG が安全に使える領域（Safe Zone）

- 間違えてもいい FAQ 回答生成
- ドキュメント要約
- 顧客問い合わせの一次分類（回答ではなく分類のみ）
- アイデア生成（正解のない創造的なもの）
- 読み取り専用の情報参照（検索補助的な扱い）

**誤答のコストが低いか、LLM が意思決定しないこと**が条件です。  
逆に Safe Zone に近いものは、「間違っても笑ってやり直せるか？」「人間があとから簡単に直せるか？」といった観点で考えるとイメージしやすくなります。

迷ったときは、

1. この回答が 100% 間違っていても問題ないか？
2. 間違いに気づける仕組み（レビューやログ）があるか？
3. それでも自動返答してよいと思えるか？

の 3 点を自問してみてください。1 つでも不安なら形式レイヤを挟むべきサインです。

---

## 曖昧性を抑える「形式レイヤ」の役割

次のような形式システムは、曖昧性を完全に排除できます。

- **SQL**（制約・整合性の保証）
- **KG**（関係性の明示と推論）
- **ルールエンジン**（IF-THEN の決定ロジック）
- **制約ソルバ**（条件下での最適解探索）

RAG と異なり、「似ているから返す」ことはありません。  
**条件に一致したデータのみを返します。**

これら形式レイヤは LLM 単体では扱えない「決定性・整合性・制約」を提供しますが、実際のシステムでは **これらを LLM にそのまま見せるのではなく、安全に公開する必要があります。ここで重要になるのが MCP（Model Context Protocol）です。**

---

### 形式レイヤの種類（概要のみ）

本記事では詳細な設計や実装パターンには踏み込みませんが、曖昧性を抑える「形式レイヤ」は大きく次の 4 つに分けられます。

- SQL：台帳・金額・在庫など、正確な値と整合性が重要なデータの管理に向く
- KG（ナレッジグラフ）：「誰が・何が・どう繋がっているか」といった関係性を辿るのに向く
- ルールエンジン：アクセス制御や承認フロー、SLA など、IF-THEN で書けるビジネスルール向け
- 制約ソルバ：スケジューリングやリソース割当など、組合せ・最適化問題向け

実システムでは、これらをすべて一度に入れる必要はなく、「LLM に決定させたくない部分」を洗い出し、そこだけでも形式レイヤに逃がすところから始めるのが現実的です。各レイヤの設計・実装パターンは別記事で掘り下げる予定です。

---

### MCP（Model Context Protocol）の位置づけと役割

MCP（Model Context Protocol）は、LLM と外部システムの間に**安全な境界（インターフェース）**を定義するためのプロトコルです。

重要なのは、次のような点を宣言的に管理できることです。

- LLM が「何にアクセスできるか」を制御する（権限制御）
- 外部システム呼び出しの入力／出力スキーマを定義する（型安全）
- LLM は宣言された範囲内のツールだけを利用できるようにする（サンドボックス化）

SQL / KG / ルールエンジン / 制約ソルバのような形式レイヤをそのまま LLM に触れさせると、権限や整合性のリスクが大きくなります。MCP を使うことで、これらを「読み取り専用 API」「判定専用 API」など、安全な窓口として公開しやすくなります。詳細な設計例や KG との連携パターンは既存記事「MCP の課題とナレッジグラフ」で扱っています。

---

### MCP が生み出すもう一つの可能性：標準化とガバナンス

MCP は単に技術的な接続手段ではなく、ツールエコシステムやガバナンスの観点でも重要です。

- LLM プラグイン／ツール呼び出し方式を標準化するレイヤになり得る
- 監査ログを取りやすく、コンプライアンス要件と相性が良い
- 大規模組織での「安全な LLM 自動化」の共通基盤になり得る

本記事では概要の紹介に留め、実運用レベルの設計は別記事に分離します。

---

## RAG の落とし穴：よくある症状とアンチパターン

RAG を実務で触ると、ほぼ全員が次のような“あるある”に遭遇します。  
技術書にはあまり載っていませんが、現場では非常に重要な視点になります。

### 1. 全員に同じ回答が返ってしまう（パーソナライズが扱いにくい問題）

素朴に RAG だけで設計すると、「質問 → 類似文書検索」という仕組み上、  
**質問者の属性・役割・履歴を扱いにくくなります。**

そのため、

- 新人でもベテランでも同じ回答
- 経理部でもカスタマーサクセスでも同じ回答
- 重要顧客でも一般顧客でも同じ回答

という“無個性な回答”になります。

**なぜか？**  
RAG は質問テキストだけをベクトル化するため、  
**「誰が聞いたか」や「その人に必要な前提知識」** を反映させる場所がありません。

**対策：**

- 質問者属性を KG / SQL で取り出し、クエリ側に事前統合する
- 「質問者プロフィール → 適切な文書セット」のルーティング層を作る
- MCP で「質問者情報を読むための安全な API」を LLM に提供する

もちろん user_id や属性情報をメタデータやプロンプトに載せてパーソナライズする設計もありますが、Embedding 空間では似た属性が近づいてしまうこともあり、「誰の文脈か」を厳密に分離するのはそれなりに難易度が高いです。

---

### 2. 回答がユーザごとに一貫せず、ばらつく

RAG は Retrieval の揺らぎ＋ LLM のサンプリングにより、

- 昨日は正しかったのに今日は変
- 朝は良かったのに夕方は悪化
- 同じ質問でも微妙に違う答え

が発生します。

これは構造上避けられません。

**対策：**

- Retrieval を決定的（非ランダム）にする
- temperature を低めに固定
- 重要な回答は形式レイヤで確定 → LLM は説明だけ
- 重要質問は「テンプレ回答」に寄せてばらつきを抑える

---

### 3. RAG では個人に紐づく問い合わせができない

実務では、

- 「このユーザの前回の問い合わせ履歴は？」
- 「この顧客の契約プランを踏まえて回答して」
- 「このチケットの担当者は？」

のような“個人・組織・履歴ベース”の質問が多いですが、  
RAG 単体ではこれらを処理できません。

理由は明確で、  
RAG が扱うのは **テキスト断片（チャンク）と埋め込み** だけであり、  
**ユーザ ID・顧客 ID・チケット ID の概念が存在しない** ためです。

**対策：**

- 個人・履歴情報は必ず SQL または KG で管理
- LLM →（ユーザ ID 抽出）→ SQL/KG → 結果を LLM が説明
- MCP で「ユーザ情報 API」「顧客情報 API」を安全に公開する

---

### 4. マニュアルの “正しい部分” ではなく “似ている部分” を拾ってしまう

- 正しい章をスキップ
- 別機能の説明を拾う
- 古いバージョンの手順を拾う

など、“似ているだけの誤文書” を拾うのは RAG の典型的事故です。

**対策：**

- メタデータ（日時・バージョン・機能名）を厳密に利用
- 検索時にフィルタリングを強制
- チャンク単位の見直し（FAQ・仕様書・手順書で単位を変える）

---

これらの「RAG あるある」を回避するには、  
RAG を“万能検索”として扱うのではなく、  
**前段にルーティング層、後段に形式レイヤ** を組み合わせる設計が不可欠です。

### 設計レベルのアンチパターンと失敗例

- LLM にそのまま判断させる  
  → もっともらしいが誤った結論を返し、後工程で混乱が発生する。
- RAG だけで高精度を期待する  
  → 検索の曖昧さ＋ LLM の曖昧さが合成され、逆に誤答が増える。
- 形式レイヤを導入せず、プロンプト改善だけで頑張る  
  → チューニングコストが際限なく膨らむ（PoC〜β 版で特に起きがち）。
- SQL やルールの“唯一の真実”を LLM 側で再生成しようとする  
  → 一貫性が壊れ、レビュー不能になる。
- 根拠リンクを“信用して”チェックを省略する  
  → 実際にはリンク自体が誤っていることがある。

---

## おわりに：LLM / RAG は強力ですが、すべてを支える“唯一の基盤”にはできません

LLM や RAG は強力なツールで、UX を大きく変革します。  
しかし、**事実・整合性・安全性を支える基盤として設計されていません**。

- LLM は確率的な生成器
- RAG は統計的に近い文書を返す仕組み
- 両者とも真実性の保証とは無関係です

だからこそ、

> **LLM / RAG を万能の基盤と誤解せず、曖昧な自然言語の入り口と出口として位置づけ、  
> その内側に形式的なレイヤをしっかり持つべきです。**

なお、近年は LLM とこれら形式レイヤの間に「安全な境界」を設けるための標準プロトコルとして **MCP（Model Context Protocol）** が登場しており、プロダクト設計における重要度が増しています（詳細は既存記事: https://zenn.dev/knowledge_graph/articles/mcp-knowledge-graph を参照してください）。

本記事と続編「LLM/RAG の曖昧性を抑える『形式レイヤ』の実装ガイド」 (/Users/takanorisuzuki/work/projects/tech-blog/articles/formal-layer-llm-rag-2025-11.md) を合わせて読むことで、LLM/RAG の限界から形式レイヤの具体実装まで一通り理解できる構成になっています。

---

## 参考文献

- Zenn: ナレッジグラフ入門  
  https://zenn.dev/knowledge_graph/articles/knowledge-graph-intro
- Zenn: RAG なしで始めるナレッジグラフ QA  
  https://zenn.dev/knowledge_graph/articles/kg-no-rag-starter
- Zenn: GenAI Divide とナレッジグラフ  
  https://zenn.dev/knowledge_graph/articles/genai-divide-knowledge-graph
- Zenn: RAG を超える知識統合  
  https://zenn.dev/knowledge_graph/articles/beyond-rag-knowledge-graph
- Zenn: MCP の課題とナレッジグラフ
  https://zenn.dev/knowledge_graph/articles/mcp-knowledge-graph

- RAG Survey: "A Survey on Retrieval-Augmentation Generation (RAG)", 2025  
  https://link.springer.com/article/10.1007/s00521-025-11666-9
- RAG Evaluation Survey: "Evaluation of Retrieval-Augmented Generation: A Survey", 2024  
  https://arxiv.org/abs/2405.07437
- LLM Hallucination Survey: "A Survey on Hallucination in Large Language Models", 2024  
  https://arxiv.org/abs/2311.05232
- Automatic Evaluation: "Evaluating Correctness and Faithfulness of Instruction-Following Models", 2024  
  https://aclanthology.org/2024.tacl-1.38/
- Faithfulness / UQ: "Faithfulness-Aware Uncertainty Quantification for Fact Verification of LLMs", 2025  
  https://arxiv.org/abs/2505.21072
- Cleanlab Blog: Real-Time Evaluation Models for RAG / hallucination detection 比較  
  https://cleanlab.ai/blog/rag-evaluation-models/

---

## 更新履歴

- 2025-11-25 — 初版作成

## 注記

本記事は AI を用いて執筆されています。内容に誤りや追加情報がある場合は、Zenn のコメントよりお知らせください。
